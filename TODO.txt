need a crawler that eats up urls from a config files that will contain the keywords to search by and need a other defining
meta data that will determine what purpose the crawler fulfills. such as measuring negativity in news sites across media outlets about hungary
so we create datasets from the found knowledge and put it into charts. we also measure other factors that will be saved regardless. like most ofter used words
and connections, frequency of mentions, potential collerations.

in terms of design:
we will need well separated service from the  config file so that the application can be repurposed by means of config inputs
we probably need to use worker threads or cluster module to speed up performance.

at the end a cronjob that runs the process on a regular intervals

later figure its potential business usage.
